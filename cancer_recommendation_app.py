import streamlit as st
import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import OneHotEncoder
import os
import matplotlib as mpl
import matplotlib.font_manager as fm
import matplotlib.pyplot as plt
import seaborn as sns
import time


# è®¾ç½®æ”¯æŒä¸­æ–‡çš„å­—ä½“
try:
    # Windows ç³»ç»Ÿ
    font_path = 'C:/Windows/Fonts/simhei.ttf'  # é»‘ä½“
    if os.path.exists(font_path):
        font_prop = fm.FontProperties(fname=font_path)
        plt.rcParams['font.family'] = font_prop.get_name()
    else:
        # å°è¯•å…¶ä»–å¸¸è§ä¸­æ–‡å­—ä½“
        for font in ['Microsoft YaHei', 'SimHei', 'KaiTi', 'SimSun']:
            if font in [f.name for f in fm.fontManager.ttflist]:
                plt.rcParams['font.family'] = font
                break
    
    # ç¡®ä¿è´Ÿå·æ­£å¸¸æ˜¾ç¤º
    plt.rcParams['axes.unicode_minus'] = False
except:
    # å¦‚æœæ‰¾ä¸åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨æ”¯æŒä¸­æ–‡çš„æ›¿ä»£æ–¹æ¡ˆ
    import matplotlib.pyplot as plt
    plt.rcParams['font.family'] = 'sans-serif'
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans']  # ä½¿ç”¨ DejaVu Sans ä½œä¸ºå¤‡é€‰
    st.warning("æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå›¾è¡¨ä¸­çš„ä¸­æ–‡å¯èƒ½æ˜¾ç¤ºå¼‚å¸¸")

# è®¾ç½®é¡µé¢
st.set_page_config(page_title="è‚¿ç˜¤èµ„è®¯ç²¾å‡†æ¨èç³»ç»Ÿ", layout="wide", page_icon="âš•ï¸")
st.title("âš•ï¸ è‚¿ç˜¤åŒ»ç”Ÿä¸“å±èµ„è®¯æ¨èç³»ç»Ÿ")
st.markdown("""
    <style>
        .reportview-container {
            margin-top: -2em;
        }
        #MainMenu {visibility: hidden;}
        .stDeployButton {display:none;}
        footer {visibility: hidden;}
        .st-eb {
            background-color: #f0f2f6;
            border-radius: 10px;
            padding: 15px;
        }
    </style>
""", unsafe_allow_html=True)

# ======================
# 1. æ¨¡æ‹Ÿæ•°æ®
# ======================
# è‚¿ç˜¤ç±»å‹åˆ—è¡¨
cancer_types = ["è‚ºç™Œ", "ä¹³è…ºç™Œ", "ç»“ç›´è‚ ç™Œ", "èƒƒç™Œ", "è‚ç™Œ", "èƒ°è…ºç™Œ", "æ·‹å·´ç˜¤", "ç™½è¡€ç—…", 
                "å‰åˆ—è…ºç™Œ", "åµå·¢ç™Œ", "å®«é¢ˆç™Œ", "è†€èƒ±ç™Œ", "è‚¾ç™Œ", "ç”²çŠ¶è…ºç™Œ", "é¼»å’½ç™Œ"]

# ç§‘å®¤åˆ—è¡¨
departments = ["è‚¿ç˜¤å†…ç§‘", "è‚¿ç˜¤å¤–ç§‘", "æ”¾å°„æ²»ç–—ç§‘", "è‚¿ç˜¤ä»‹å…¥ç§‘", "ç—…ç†ç§‘", "å½±åƒç§‘", "æ ¸åŒ»å­¦ç§‘"]

# åˆ›å»ºæ¨¡æ‹ŸåŒ»ç”Ÿæ•°æ®
def generate_doctors(num=100):
    np.random.seed(42)
    data = {
        "doctor_id": range(1, num+1),
        "name": [f"åŒ»ç”Ÿ{str(i).zfill(3)}" for i in range(1, num+1)],
        "level": np.random.choice(["ä½é™¢åŒ»å¸ˆ", "ä¸»æ²»åŒ»å¸ˆ", "å‰¯ä¸»ä»»åŒ»å¸ˆ", "ä¸»ä»»åŒ»å¸ˆ"], num, p=[0.2, 0.4, 0.3, 0.1]),
        "hospital_level": np.random.choice(["ç¤¾åŒºåŒ»é™¢", "äºŒçº§åŒ»é™¢", "ä¸‰ä¹™åŒ»é™¢", "ä¸‰ç”²åŒ»é™¢"], num, p=[0.1, 0.3, 0.3, 0.3]),
        "department": np.random.choice(departments, num),
        "specialty": [np.random.choice(cancer_types, np.random.randint(1, 4), replace=False).tolist() for _ in range(num)],
        "years_exp": np.random.randint(1, 35, num)
    }
    return pd.DataFrame(data)

# åˆ›å»ºæ¨¡æ‹Ÿæ–‡ç« æ•°æ®
# åˆ›å»ºæ›´ä¸°å¯Œçš„æ¨¡æ‹Ÿæ–‡ç« æ•°æ®
def generate_articles(num=200):
    np.random.seed(42)
    
    # æ›´ä¸°å¯Œçš„æ–‡ç« ç±»å‹
    article_types = ["ä¸´åºŠç ”ç©¶", "æŒ‡å—æ›´æ–°", "ç—…ä¾‹è®¨è®º", "ç»¼è¿°", "ä¸´åºŠè¯•éªŒ", "æŠ€æœ¯è¿›å±•", "ä¸“å®¶å…±è¯†", 
                    "ç ”ç©¶ç®€æŠ¥", "æ²»ç–—æ–°ç­–ç•¥", "è¯ç‰©è¯„ä»·", "è¯Šæ–­çªç ´", "é¢„åæ¨¡å‹", "å¤šä¸­å¿ƒç ”ç©¶"]
    
    # æ›´ä¸°å¯Œçš„æ ‡é¢˜å‰ç¼€
    title_prefixes = [
        "è‚¿ç˜¤é¢†åŸŸé‡è¦è¿›å±•ï¼š", "æœ€æ–°ç ”ç©¶ï¼š", "çªç ´æ€§å‘ç°ï¼š", "ä¸´åºŠå®è·µï¼š", "ä¸“å®¶è§†è§’ï¼š", 
        "æŠ€æœ¯å‰æ²¿ï¼š", "æ²»ç–—æ–°é€‰æ‹©ï¼š", "è¯Šæ–­æ–°æ–¹æ³•ï¼š", "é¢„åè¯„ä¼°ï¼š", "ç—…ä¾‹åˆ†äº«ï¼š", 
        "æŒ‡å—è§£è¯»ï¼š", "ç»¼è¿°ï¼š", "å¤šä¸­å¿ƒç ”ç©¶ï¼š", "çœŸå®ä¸–ç•Œç ”ç©¶ï¼š", "åˆ†å­æœºåˆ¶æ¢ç´¢ï¼š"
    ]
    
    # æ›´ä¸°å¯Œçš„ä¸»é¢˜å…³é”®è¯
    themes = [
        "é¶å‘æ²»ç–—", "å…ç–«æ²»ç–—", "ç²¾å‡†æ”¾ç–—", "æ—©æœŸç­›æŸ¥", "åˆ†å­åˆ†å‹", 
        "è€è¯æœºåˆ¶", "ç”Ÿç‰©æ ‡å¿—ç‰©", "åŸºå› æ£€æµ‹", "æ¶²ä½“æ´»æ£€", "è‚¿ç˜¤å¾®ç¯å¢ƒ",
        "è”åˆæ²»ç–—", "æ–°è¾…åŠ©æ²»ç–—", "è¾…åŠ©æ²»ç–—", "å§‘æ¯æ²»ç–—", "ç”Ÿå­˜è´¨é‡",
        "å‰¯ä½œç”¨ç®¡ç†", "æ‚£è€…æŠ¥å‘Šç»“å±€", "æˆæœ¬æ•ˆç›Š", "çœŸå®ä¸–ç•Œè¯æ®", "è½¬åŒ–ç ”ç©¶"
    ]
    
    # æ›´ä¸°å¯Œçš„åç¼€æè¿°
    suffixes = [
        "çš„ä¸´åºŠå®è·µ", "çš„ç ”ç©¶è¿›å±•", "çš„çªç ´æ€§å‘ç°", "çš„å¤šä¸­å¿ƒç ”ç©¶", "çš„ç³»ç»Ÿè¯„ä»·",
        "çš„é•¿æœŸéšè®¿ç»“æœ", "çš„èŸèƒåˆ†æ", "çš„å‰ç»æ€§ç ”ç©¶", "çš„å›é¡¾æ€§åˆ†æ", "çš„ä¸“å®¶å…±è¯†",
        "çš„æ²»ç–—ç­–ç•¥", "çš„è¯Šæ–­ä»·å€¼", "çš„é¢„åæ„ä¹‰", "çš„æœºåˆ¶ç ”ç©¶", "çš„åº”ç”¨å‰æ™¯"
    ]
    
    data = {
        "article_id": range(1, num+1),
        "title": [],
        "type": np.random.choice(article_types, num),
        "cancer_type": [np.random.choice(cancer_types, np.random.randint(1, 3), replace=False).tolist() for _ in range(num)],
        "department": [np.random.choice(departments, np.random.randint(1, 3), replace=False).tolist() for _ in range(num)],
        "authority": np.random.choice(["å›½é™…é¡¶çº§æœŸåˆŠ", "å›½å†…æ ¸å¿ƒæœŸåˆŠ", "å­¦ä¼šæŒ‡å—", "ä¼šè®®æ‘˜è¦", "é¢„å°æœ¬"], num, p=[0.1, 0.4, 0.2, 0.2, 0.1]),
        "impact_factor": np.random.uniform(0, 30, num).round(1),
        "pub_date": pd.date_range(start="2023-01-01", end="2024-05-01", periods=num),
        "keywords": [[np.random.choice(themes, np.random.randint(3, 6), replace=False).tolist()] for _ in range(num)]
    }
    
    # ç”Ÿæˆå¤šæ ·åŒ–çš„æ ‡é¢˜
    for i in range(num):
        cancer = np.random.choice(cancer_types)
        theme = np.random.choice(themes)
        
        # éšæœºé€‰æ‹©æ ‡é¢˜ç»“æ„
        title_type = np.random.choice(["prefix", "simple", "question", "numbered"], p=[0.4, 0.3, 0.2, 0.1])
        
        if title_type == "prefix":
            prefix = np.random.choice(title_prefixes)
            title = f"{prefix}{cancer}{theme}{suffixes[i % len(suffixes)]}"
        elif title_type == "simple":
            title = f"{cancer}{theme}çš„æœ€æ–°ç ”ç©¶è¿›å±•"
        elif title_type == "question":
            questions = [
                f"{cancer}æ²»ç–—ï¼š{theme}æ˜¯å¦å¸¦æ¥ç”Ÿå­˜è·ç›Šï¼Ÿ",
                f"å¦‚ä½•ä¼˜åŒ–{cancer}çš„{theme}ç­–ç•¥ï¼Ÿ",
                f"{theme}åœ¨{cancer}æ²»ç–—ä¸­çš„ä»·å€¼å‡ ä½•ï¼Ÿ"
            ]
            title = np.random.choice(questions)
        else:  # numbered
            title = f"ç ”ç©¶æŠ¥é“{(i+1)}ï¼š{theme}åœ¨{cancer}ä¸­çš„åº”ç”¨"
        
        data["title"].append(title)
    
    return pd.DataFrame(data)

# ç”Ÿæˆæ•°æ®
doctors = generate_doctors(100)
articles = generate_articles(200)

# ======================
# 2. ç‰¹å¾å·¥ç¨‹
# ======================
# ç‰¹å¾ç¼–ç 
def encode_features(doctors_df, articles_df):
    # åŒ»ç”Ÿç‰¹å¾ç¼–ç 
    doctor_features = pd.DataFrame()
    
    # èŒçº§ç¼–ç 
    level_map = {"ä½é™¢åŒ»å¸ˆ": 1, "ä¸»æ²»åŒ»å¸ˆ": 2, "å‰¯ä¸»ä»»åŒ»å¸ˆ": 3, "ä¸»ä»»åŒ»å¸ˆ": 4}
    doctor_features["level"] = doctors_df["level"].map(level_map).fillna(0)
    
    # åŒ»é™¢ç­‰çº§ç¼–ç 
    hospital_map = {"ç¤¾åŒºåŒ»é™¢": 1, "äºŒçº§åŒ»é™¢": 2, "ä¸‰ä¹™åŒ»é™¢": 3, "ä¸‰ç”²åŒ»é™¢": 4}
    doctor_features["hospital_level"] = doctors_df["hospital_level"].map(hospital_map).fillna(0)
    
    # è·å–æ‰€æœ‰ç§‘å®¤å’Œç˜¤è‚¿çš„ç»Ÿä¸€åˆ—è¡¨
    all_departments = list(set(doctors_df['department'].unique().tolist() + 
                             [dept for sublist in articles_df['department'] for dept in sublist]))
    
    all_cancers = list(set([c for sublist in doctors_df['specialty'] for c in sublist] + 
                           [c for sublist in articles_df['cancer_type'] for c in sublist]))
    
    # ç§‘å®¤OneHotç¼–ç  - ä½¿ç”¨ç»Ÿä¸€çš„ç»´åº¦
    dept_encoder = OneHotEncoder(categories=[all_departments], sparse_output=False, handle_unknown='ignore')
    # å…ˆæ‹Ÿåˆç©ºæ•°æ®ä»¥åˆ›å»ºcategories_å±æ€§
    dept_encoder.fit(doctors_df[["department"]])
    dept_encoded = dept_encoder.transform(doctors_df[["department"]])
    dept_cols = [f"dept_{dept}" for dept in dept_encoder.categories_[0]]
    doctor_features[dept_cols] = dept_encoded
    
    # æ“…é•¿ç˜¤è‚¿ç¼–ç  - ä½¿ç”¨ç»Ÿä¸€çš„ç»´åº¦
    cancer_encoder = OneHotEncoder(categories=[all_cancers], sparse_output=False, handle_unknown='ignore')
    # å…ˆæ‹Ÿåˆç©ºæ•°æ®ä»¥åˆ›å»ºcategories_å±æ€§
    cancer_encoder.fit(doctors_df[["department"]])  # ä»»æ„æ•°æ®ï¼Œåªä¸ºåˆå§‹åŒ–
    
    cancer_features = []
    for specialties in doctors_df["specialty"]:
        vec = np.zeros(len(cancer_encoder.categories_[0]))
        for spec in specialties:
            if spec in cancer_encoder.categories_[0]:
                idx = np.where(cancer_encoder.categories_[0] == spec)[0][0]
                vec[idx] = 1
        cancer_features.append(vec)
    
    cancer_cols = [f"cancer_{c}" for c in cancer_encoder.categories_[0]]
    doctor_features[cancer_cols] = cancer_features
    
    # å·¥ä½œç»éªŒæ ‡å‡†åŒ– - æ·»åŠ ç¼ºå¤±å€¼å¤„ç†
    if doctors_df["years_exp"].max() > 0:
        doctor_features["years_exp"] = doctors_df["years_exp"] / doctors_df["years_exp"].max()
    else:
        doctor_features["years_exp"] = 0  # å¤„ç†é™¤é›¶é”™è¯¯
    
    # æ–‡ç« ç‰¹å¾ç¼–ç 
    article_features = pd.DataFrame()
    
    # æ–‡ç« ç±»å‹OneHotç¼–ç 
    all_article_types = articles_df['type'].unique().tolist()
    type_encoder = OneHotEncoder(categories=[all_article_types], sparse_output=False, handle_unknown='ignore')
    type_encoder.fit(articles_df[["type"]])  # æ‹Ÿåˆä»¥åˆ›å»ºcategories_å±æ€§
    type_encoded = type_encoder.transform(articles_df[["type"]])
    type_cols = [f"type_{t}" for t in type_encoder.categories_[0]]
    article_features[type_cols] = type_encoded
    
    # æ–‡ç« ç˜¤è‚¿ç±»å‹ç¼–ç  - ä½¿ç”¨ç»Ÿä¸€çš„ç»´åº¦
    cancer_features_art = []
    for cancers in articles_df["cancer_type"]:
        vec = np.zeros(len(cancer_encoder.categories_[0]))
        for cancer in cancers:
            if cancer in cancer_encoder.categories_[0]:
                idx = np.where(cancer_encoder.categories_[0] == cancer)[0][0]
                vec[idx] = 1
        cancer_features_art.append(vec)
    
    article_features[cancer_cols] = cancer_features_art
    
    # æ–‡ç« ç§‘å®¤ç¼–ç  - ä½¿ç”¨ç»Ÿä¸€çš„ç»´åº¦
    dept_features_art = []
    for depts in articles_df["department"]:
        vec = np.zeros(len(dept_encoder.categories_[0]))
        for dept in depts:
            if dept in dept_encoder.categories_[0]:
                idx = np.where(dept_encoder.categories_[0] == dept)[0][0]
                vec[idx] = 1
        dept_features_art.append(vec)
    
    article_features[dept_cols] = dept_features_art
    
    # æƒå¨æ€§ç¼–ç 
    authority_map = {"ä¼šè®®æ‘˜è¦": 1, "å›½å†…æ ¸å¿ƒæœŸåˆŠ": 2, "å­¦ä¼šæŒ‡å—": 3, "å›½é™…é¡¶çº§æœŸåˆŠ": 4}
    article_features["authority"] = articles_df["authority"].map(authority_map).fillna(0)
    
    # å½±å“å› å­æ ‡å‡†åŒ– - æ·»åŠ ç¼ºå¤±å€¼å¤„ç†
    max_impact = articles_df["impact_factor"].max()
    if max_impact > 0:
        article_features["impact_factor"] = articles_df["impact_factor"] / max_impact
    else:
        article_features["impact_factor"] = 0
    
    # æ—¶é—´è¡°å‡å› å­ - æ·»åŠ ç¼ºå¤±å€¼å¤„ç†
    max_date = articles_df["pub_date"].max()
    if not pd.isnull(max_date):
        article_features["recency"] = (max_date - articles_df["pub_date"]).dt.days.apply(
            lambda x: np.exp(-x/180) if not pd.isnull(x) else 0
        )
    else:
        article_features["recency"] = 0
    
    # ç¡®ä¿åŒ»ç”Ÿå’Œæ–‡ç« ç‰¹å¾ç»´åº¦ä¸€è‡´
    # æ·»åŠ ç¼ºå¤±çš„åˆ—å¹¶å¡«å……0
    for col in doctor_features.columns:
        if col not in article_features:
            article_features[col] = 0
    
    for col in article_features.columns:
        if col not in doctor_features:
            doctor_features[col] = 0
    
    # ç¡®ä¿åˆ—é¡ºåºä¸€è‡´
    doctor_features = doctor_features[article_features.columns]
    
    # å¤„ç†ç©ºå€¼é—®é¢˜ - ç¡®ä¿æ²¡æœ‰ NaN
    doctor_features = doctor_features.fillna(0)
    article_features = article_features.fillna(0)
    
    # éªŒè¯æ²¡æœ‰ NaN
    if doctor_features.isnull().any().any() or article_features.isnull().any().any():
        st.error("è­¦å‘Šï¼šç‰¹å¾çŸ©é˜µä¸­å­˜åœ¨ç©ºå€¼ï¼")
        st.write("åŒ»ç”Ÿç‰¹å¾ç©ºå€¼:", doctor_features.isnull().sum().sum())
        st.write("æ–‡ç« ç‰¹å¾ç©ºå€¼:", article_features.isnull().sum().sum())
    
    return doctor_features, article_features, dept_cols, cancer_cols, type_cols

# æ‰§è¡Œç‰¹å¾ç¼–ç 
doctor_features, article_features, dept_cols, cancer_cols, type_cols = encode_features(doctors, articles)

# ======================
# 3. æ¨èç®—æ³•
# ======================
def recommend_articles(doctor_id, top_n=5):
    """ä¸ºæŒ‡å®šåŒ»ç”Ÿæ¨èæ–‡ç« """
    doctor_idx = doctor_id - 1
    
    # è·å–åŒ»ç”Ÿå‘é‡
    doctor_vec = doctor_features.iloc[doctor_idx].values.reshape(1, -1)
    
    # æ–°å¢ï¼šéªŒè¯å‘é‡æ²¡æœ‰ NaN
    if np.isnan(doctor_vec).any():
        st.error(f"åŒ»ç”Ÿå‘é‡åŒ…å«NaNå€¼: {doctor_idx}")
        st.write(doctor_vec)
        doctor_vec = np.nan_to_num(doctor_vec)
    
    # è·å–æ–‡ç« ç‰¹å¾çŸ©é˜µ
    article_vecs = article_features.values
    
    # æ–°å¢ï¼šéªŒè¯çŸ©é˜µæ²¡æœ‰ NaN
    if np.isnan(article_vecs).any():
        st.error("æ–‡ç« ç‰¹å¾çŸ©é˜µåŒ…å«NaNå€¼ï¼")
        # è®¡ç®—æ¯åˆ—çš„NaNæ•°é‡
        nan_counts = np.isnan(article_vecs).sum(axis=0)
        st.write("æ¯åˆ—NaNæ•°é‡:", nan_counts)
        article_vecs = np.nan_to_num(article_vecs)
    
    # è®¡ç®—ç”¨æˆ·å‘é‡ä¸æ‰€æœ‰æ–‡ç« å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
    similarity_scores = cosine_similarity(doctor_vec, article_vecs)[0]
    
    # è·å–æ¨èæ’åº
    ranked_articles = np.argsort(similarity_scores)[::-1][:top_n]
    
    # è¿”å›æ¨èç»“æœå’Œç›¸ä¼¼åº¦åˆ†æ•°
    recommended = articles.iloc[ranked_articles].copy()
    recommended["match_score"] = similarity_scores[ranked_articles].round(3)
    
    return recommended

# ======================
# 4. Streamlitäº¤äº’ç•Œé¢
# ======================
# ä¾§è¾¹æ  - ç”¨æˆ·é€‰æ‹©
st.sidebar.header("ğŸ‘¨â€âš•ï¸ åŒ»ç”Ÿä¿¡æ¯è®¾ç½®")
selected_doctor = st.sidebar.selectbox("é€‰æ‹©åŒ»ç”Ÿæ¡£æ¡ˆ", doctors["name"])

# è·å–é€‰å®šåŒ»ç”Ÿçš„ä¿¡æ¯
doctor_info = doctors[doctors["name"] == selected_doctor].iloc[0]

# æ˜¾ç¤ºåŒ»ç”Ÿä¿¡æ¯
st.sidebar.subheader("åŒ»ç”Ÿæ¡£æ¡ˆè¯¦æƒ…")
st.sidebar.markdown(f"**èŒçº§:** {doctor_info['level']}")
st.sidebar.markdown(f"**åŒ»é™¢ç­‰çº§:** {doctor_info['hospital_level']}")
st.sidebar.markdown(f"**ç§‘å®¤:** {doctor_info['department']}")
st.sidebar.markdown(f"**æ“…é•¿ç˜¤è‚¿:** {', '.join(doctor_info['specialty'])}")  
st.sidebar.markdown(f"**å·¥ä½œç»éªŒ:** {doctor_info['years_exp']}å¹´")

# æ¨èè®¾ç½®
st.sidebar.subheader("æ¨èè®¾ç½®")
top_n = st.sidebar.slider("æ¨èæ–‡ç« æ•°é‡", 3, 10, 5)
weight_specialty = st.sidebar.slider("æ“…é•¿ç˜¤è‚¿æƒé‡", 0.0, 1.0, 0.6)
weight_department = st.sidebar.slider("ç§‘å®¤æƒé‡", 0.0, 1.0, 0.3)
weight_level = st.sidebar.slider("èŒçº§æƒé‡", 0.0, 1.0, 0.1)

# è°ƒæ•´æƒé‡
doctor_features_weighted = doctor_features.copy()
doctor_features_weighted[cancer_cols] *= weight_specialty
doctor_features_weighted[dept_cols] *= weight_department
doctor_features_weighted["level"] *= weight_level

# ä¸»ç•Œé¢
st.header(f"ğŸ“° ä¸º {selected_doctor} æ¨èçš„è‚¿ç˜¤èµ„è®¯")

# æ·»åŠ è¿›åº¦æ¡
progress_bar = st.progress(0)
status_text = st.empty()

# æ¨¡æ‹Ÿè®¡ç®—è¿‡ç¨‹
for i in range(100):
    progress_bar.progress(i + 1)
    status_text.text(f"æ­£åœ¨åŒ¹é…æœ€ä½³èµ„è®¯... {i+1}%")
    time.sleep(0.01)

# è·å–æ¨èç»“æœ
recommended_articles = recommend_articles(doctor_info["doctor_id"], top_n)

# æ˜¾ç¤ºæ¨èç»“æœ
for idx, row in recommended_articles.iterrows():
    with st.expander(f"**[{row['article_id']}] {row['title']}** (åŒ¹é…åº¦: {row['match_score']:.2f})", expanded=idx==0):
        col1, col2 = st.columns([1, 3])
        
        with col1:
            # æ–‡ç« ç±»å‹å›¾æ ‡
            type_icon = {
                "ä¸´åºŠç ”ç©¶": "ğŸ”¬",
                "æŒ‡å—æ›´æ–°": "ğŸ“œ",
                "ç—…ä¾‹è®¨è®º": "ğŸ“‹",
                "ç»¼è¿°": "ğŸ“š",
                "ä¸´åºŠè¯•éªŒ": "ğŸ’Š",
                "æŠ€æœ¯è¿›å±•": "âš™ï¸",
                "ä¸“å®¶å…±è¯†": "ğŸ‘¥",
                "ç ”ç©¶ç®€æŠ¥": "ğŸ“°",
                "æ²»ç–—æ–°ç­–ç•¥": "ğŸ’¡",
                "è¯ç‰©è¯„ä»·": "ğŸ’Š",
                "è¯Šæ–­çªç ´": "ğŸ”",
                "é¢„åæ¨¡å‹": "ğŸ“Š",
                "å¤šä¸­å¿ƒç ”ç©¶": "ğŸŒ"
            }.get(row["type"], "ğŸ“„")
            
            st.markdown(f"**ç±»å‹:** {type_icon} {row['type']}")
            st.markdown(f"**ç›¸å…³ç˜¤è‚¿:** {', '.join(row['cancer_type'])}")
            st.markdown(f"**ç›¸å…³ç§‘å®¤:** {', '.join(row['department'])}")
            st.markdown(f"**å‘è¡¨æ¥æº:** {row['authority']}")
            st.markdown(f"**å½±å“å› å­:** {row['impact_factor']}")
            st.markdown(f"**å‘å¸ƒæ—¥æœŸ:** {row['pub_date'].strftime('%Y-%m-%d')}")
        
        with col2:
            # ç”ŸæˆåŒ¹é…ç†ç”±
            match_reasons = []
            
            # ç§‘å®¤åŒ¹é…
            if any(dept in row["department"] for dept in [doctor_info["department"]]):
                match_reasons.append(f"ä¸æ‚¨çš„ç§‘å®¤({doctor_info['department']})ç›¸å…³")
            
            # ç˜¤è‚¿åŒ¹é…
            common_cancers = set(doctor_info["specialty"]) & set(row["cancer_type"])
            if common_cancers:
                match_reasons.append(f"æ¶‰åŠæ‚¨æ“…é•¿çš„{', '.join(common_cancers)}")
            
            # èŒçº§åŒ¹é…
            if doctor_info["level"] in ["å‰¯ä¸»ä»»åŒ»å¸ˆ", "ä¸»ä»»åŒ»å¸ˆ"] and row["type"] in ["æŒ‡å—æ›´æ–°", "ä¸“å®¶å…±è¯†", "ç»¼è¿°"]:
                match_reasons.append("ç¬¦åˆæ‚¨çš„é«˜çº§èŒç§°éœ€æ±‚")
            elif doctor_info["level"] in ["ä½é™¢åŒ»å¸ˆ", "ä¸»æ²»åŒ»å¸ˆ"] and row["type"] in ["ç—…ä¾‹è®¨è®º", "ä¸´åºŠç ”ç©¶", "æ²»ç–—æ–°ç­–ç•¥"]:
                match_reasons.append("ç¬¦åˆæ‚¨çš„ä¸´åºŠå­¦ä¹ éœ€æ±‚")
            
            # å½±å“å› å­
            if row["impact_factor"] > 15:
                match_reasons.append("æ¥è‡ªé«˜å½±å“åŠ›æœŸåˆŠ")
            
            # æ˜¾ç¤ºåŒ¹é…ç†ç”±
            if match_reasons:
                st.info("**åŒ¹é…ç†ç”±:** " + "ï¼›".join(match_reasons))
            else:
                st.info("**åŒ¹é…ç†ç”±:** ç»¼åˆç‰¹å¾åŒ¹é…")
            
            # æ˜¾ç¤ºå…³é”®è¯
            keywords = row["keywords"][0] if isinstance(row["keywords"], list) else row["keywords"]
            st.markdown("**å…³é”®è¯:** " + ", ".join(keywords))
            
            # ç”Ÿæˆæ‘˜è¦ - æ›´ä¸“ä¸šçš„åŒ»å­¦æ‘˜è¦
            cancer = np.random.choice(row["cancer_type"])
            keywords_str = ", ".join(keywords[:3])
            st.markdown(f"""
            **æ‘˜è¦:**  
            æœ¬ç ”ç©¶æ¢è®¨äº†{keywords_str}åœ¨{cancer}æ²»ç–—ä¸­çš„åº”ç”¨ã€‚é€šè¿‡å¯¹{np.random.randint(50, 500)}ä¾‹æ‚£è€…çš„åˆ†æï¼Œ
            å‘ç°{np.random.choice(["æ˜¾è‘—æé«˜ç”Ÿå­˜ç‡", "æ˜æ˜¾æ”¹å–„ç”Ÿæ´»è´¨é‡", "æœ‰æ•ˆé™ä½å¤å‘é£é™©", "å…·æœ‰è‰¯å¥½å®‰å…¨æ€§"])}ã€‚
            ç ”ç©¶ç»“æœä¸º{cancer}çš„ä¸´åºŠå®è·µæä¾›äº†æ–°çš„å¾ªè¯ä¾æ®ã€‚
            """)

# æ·»åŠ åˆ†éš”çº¿
st.markdown("---")

# æ¨èåˆ†æå›¾è¡¨ (è‹±æ–‡ç‰ˆ)
st.subheader("Recommendation Analysis")
col1, col2 = st.columns(2)

with col1:
    # æ–‡ç« ç±»å‹åˆ†å¸ƒ (è‹±æ–‡)
    try:
        fig, ax = plt.subplots(figsize=(8, 4))
        
        # è·å–ç±»å‹è®¡æ•°å¹¶æ’åº
        type_counts = recommended_articles["type"].value_counts()
        sorted_types = type_counts.index.tolist()
        
        # åˆ›å»ºæ¡å½¢å›¾
        ax.barh(sorted_types, type_counts.values, color='#1f77b4')
        
        # è®¾ç½®è‹±æ–‡æ ‡ç­¾
        ax.set_title("Distribution of Recommended Article Types")
        ax.set_xlabel("Count")
        ax.set_ylabel("Article Type")
        
        st.pyplot(fig)
    except Exception as e:
        st.error(f"Error drawing type distribution: {str(e)}")
        st.write("Distribution of Recommended Article Types:")
        st.dataframe(type_counts)

with col2:
    # åŒ¹é…åˆ†æ•°åˆ†å¸ƒ (è‹±æ–‡)
    try:
        fig, ax = plt.subplots(figsize=(8, 4))
        
        # åˆ›å»ºç›´æ–¹å›¾
        sns.histplot(recommended_articles["match_score"], bins=6, kde=True, ax=ax, color='#ff7f0e')
        
        # è®¾ç½®è‹±æ–‡æ ‡ç­¾
        ax.set_title("Distribution of Match Scores")
        ax.set_xlabel("Match Score")
        ax.set_ylabel("Number of Articles")
        
        st.pyplot(fig)
    except Exception as e:
        st.error(f"Error drawing score distribution: {str(e)}")
        st.write("Match Score Statistics:")
        st.write(recommended_articles["match_score"].describe())

# ç³»ç»Ÿè¯´æ˜
st.markdown("---")
st.subheader("ç³»ç»Ÿè¯´æ˜")
st.markdown("""
æœ¬æ¨èç³»ç»ŸåŸºäºåŒ»ç”Ÿä»¥ä¸‹ç‰¹å¾è¿›è¡Œä¸ªæ€§åŒ–æ¨èï¼š
1. **èŒçº§**ï¼šä¸åŒèŒçº§åŒ»ç”Ÿå…³æ³¨å†…å®¹ä¸åŒï¼ˆä½é™¢åŒ»å¸ˆæ›´å…³æ³¨åŸºç¡€çŸ¥è¯†ï¼Œä¸»ä»»åŒ»å¸ˆæ›´å…³æ³¨å‰æ²¿è¿›å±•ï¼‰
2. **åŒ»é™¢ç­‰çº§**ï¼šä¸åŒçº§åˆ«åŒ»é™¢å¯è·å–èµ„æºä¸åŒ
3. **ç§‘å®¤**ï¼šç²¾å‡†åŒ¹é…åŒ»ç”Ÿæ‰€åœ¨ç§‘å®¤ç›¸å…³å†…å®¹
4. **æ“…é•¿ç˜¤è‚¿**ï¼šé‡ç‚¹æ¨èåŒ»ç”Ÿä¸“é•¿é¢†åŸŸçš„èµ„è®¯
5. **å·¥ä½œç»éªŒ**ï¼šç»éªŒä¸°å¯Œçš„åŒ»ç”Ÿæ›´å…³æ³¨å¤æ‚ç—…ä¾‹å’Œå‰æ²¿ç ”ç©¶

ç³»ç»Ÿé€šè¿‡ç‰¹å¾å‘é‡åŒ–å’Œä½™å¼¦ç›¸ä¼¼åº¦ç®—æ³•è®¡ç®—åŒ¹é…åº¦ï¼Œæƒé‡å¯è°ƒã€‚
""")

# åœ¨ä¾§è¾¹æ æ·»åŠ è°ƒè¯•é€‰é¡¹
debug_mode = st.sidebar.checkbox("è°ƒè¯•æ¨¡å¼")

if debug_mode:
    st.sidebar.subheader("è°ƒè¯•ä¿¡æ¯")
    st.sidebar.write("åŒ»ç”Ÿç‰¹å¾ç»´åº¦:", doctor_features.shape)
    st.sidebar.write("æ–‡ç« ç‰¹å¾ç»´åº¦:", article_features.shape)
    
    # æ˜¾ç¤ºNaNæ•°é‡
    doctor_nan = doctor_features.isnull().sum().sum()
    article_nan = article_features.isnull().sum().sum()
    st.sidebar.write(f"åŒ»ç”Ÿç‰¹å¾NaNæ•°é‡: {doctor_nan}")
    st.sidebar.write(f"æ–‡ç« ç‰¹å¾NaNæ•°é‡: {article_nan}")
    
    if doctor_nan > 0:
        st.sidebar.write("åŒ»ç”Ÿç‰¹å¾NaNåˆ—:")
        st.sidebar.write(doctor_features.isnull().sum())
    
    if article_nan > 0:
        st.sidebar.write("æ–‡ç« ç‰¹å¾NaNåˆ—:")
        st.sidebar.write(article_features.isnull().sum())

# éšè—è¿›åº¦æ¡
progress_bar.empty()
status_text.empty()
